
import { GoogleGenAI, Type, Schema, Chat, Part } from "@google/genai";
import { AnalysisResult, VocabItem, GlossaryItem } from "../types";
import { isValidSubtitleFormat } from "../utils/srtParser";

// Helper to get client with dynamic config
const getClient = (apiKey: string, baseUrl?: string) => {
  if (!apiKey) {
    throw new Error("API Key is missing. Please configure it in Settings.");
  }
  
  const options: any = { apiKey };
  if (baseUrl && baseUrl.trim() !== "") {
    options.baseUrl = baseUrl; 
  }

  return new GoogleGenAI(options);
};

const SYSTEM_INSTRUCTION = `
You are a professional audio/video content proofreading expert and terminology manager.
Your goal is to assist users in proofreading, organizing audio transcripts, and managing technical glossaries.

Principles:
1. Integrity First: 100% of the content must be preserved unless asking for metadata.
2. Timestamp Accuracy: Adhere strictly to the provided SRT.
3. Proofreading: Correct typos, proper nouns, and technical terms.
4. Contextual Awareness: Use provided glossaries and extra context instructions.
`;

// STRICT AI OUTPUT STATUS SCHEMA
// AI should only return objective statuses. 
// 'custom' and 'ai_recheck' are strictly for USER-side state management and should never be generated by AI.
const VOCAB_ITEM_SCHEMA: Schema = {
    type: Type.OBJECT,
    properties: {
      id: { type: Type.INTEGER },
      timeRange: { type: Type.STRING },
      original: { type: Type.STRING },
      corrected: { type: Type.STRING },
      type: { type: Type.STRING },
      // AI allowed statuses:
      // corrected: You are confident in the fix.
      // needs_confirmation: You are unsure or context is ambiguous.
      // check_spelling: It looks like a typo but might be a proper noun.
      status: { type: Type.STRING, enum: ['corrected', 'needs_confirmation', 'check_spelling'] },
      aiReason: { type: Type.STRING, description: "Reason for correction or note" }
    },
    required: ["id", "timeRange", "original", "corrected", "type", "status"]
};

const ANALYSIS_RESPONSE_SCHEMA: Schema = {
    type: Type.OBJECT,
    properties: {
      summary: {
        type: Type.OBJECT,
        properties: {
          topic: { type: Type.STRING },
          speakers: { type: Type.ARRAY, items: { type: Type.STRING } },
          duration: { type: Type.STRING },
          agenda: { type: Type.ARRAY, items: { type: Type.STRING } }
        },
        required: ["topic", "speakers", "duration", "agenda"]
      },
      vocabList: {
        type: Type.ARRAY,
        items: VOCAB_ITEM_SCHEMA
      }
    },
    required: ["summary", "vocabList"]
};

/**
 * AnalysisSession: Manages the stateful conversation for a specific project.
 * Allows iterative updates without re-sending the entire file.
 */
export class AnalysisSession {
    private chat: Chat;
    private modelName: string;

    constructor(apiKey: string, baseUrl: string, modelName: string) {
        const ai = getClient(apiKey, baseUrl);
        this.modelName = modelName;
        this.chat = ai.chats.create({
            model: modelName,
            config: {
                systemInstruction: SYSTEM_INSTRUCTION,
                responseMimeType: "application/json",
                responseSchema: ANALYSIS_RESPONSE_SCHEMA
            }
        });
    }

    /**
     * Start the first analysis of the content (Multimodal Support).
     */
    async start(
        srtContent: string | null,
        mediaParts: Part[], // Array of inlineData parts (audio/video)
        language: string, 
        glossary?: GlossaryItem[],
        extraContext?: string
    ): Promise<AnalysisResult> {
        let textPrompt = `
        Step 2 - Preliminary Proofreading (Initial Analysis):
        
        Analyze the provided content (Video/Audio/Subtitle).
        IMPORTANT: The output values (summary text, type descriptions) MUST be in ${language}.
        `;

        if (glossary && glossary.length > 0) {
            textPrompt += `\nUse the following Glossary strictly for terminology consistency:\n${glossary.map(g => `- ${g.term}: ${g.definition}`).join('\n')}`;
        }

        if (extraContext) {
            textPrompt += `\nGlobal User Instructions/Context:\n"${extraContext}"`;
        }

        textPrompt += `\n\nTask:\n1. Generate a brief summary.\n2. Identify ALL proper nouns and technical terms.\n`;
        
        if (srtContent) {
            textPrompt += `\nSubtitle Content (Reference):\n${srtContent.slice(0, 30000)} ${srtContent.length > 30000 ? "...(truncated)" : ""}`;
        } else {
            textPrompt += `\n(No subtitle provided. Please analyze based on the audio/video media directly)`;
        }

        // Combine media parts with the text prompt
        const contentParts: (Part | string)[] = [...mediaParts, textPrompt];

        // For Chat.sendMessage, we pass 'message' which can be string or Part[]
        // However, the SDK types for sendMessage can be tricky. It usually expects `message: string | Part[]`.
        // We construct the payload manually to be safe.
        const response = await this.chat.sendMessage({ 
            message: contentParts 
        });
        
        if (!response.text) throw new Error("No response from AI");
        return JSON.parse(response.text) as AnalysisResult;
    }

    /**
     * Send user edits, glossary updates, and instructions back to the SAME session for re-evaluation.
     */
    async iterate(
        currentVocab: VocabItem[],
        newInstruction: string,
        language: string,
        activeGlossaryItems: GlossaryItem[] = []
    ): Promise<AnalysisResult> {
        // Construct a diff-like message
        const currentStatus = currentVocab.map(v => 
            `ID: ${v.id} | Original: "${v.original}" | UserCorrected: "${v.corrected}" | UserNote: "${v.userNote || ''}"`
        ).join('\n');

        let prompt = `
        Step 2.1 - Iterative Re-analysis (Review Mode):
        
        The user has updated the glossary or provided new instructions.
        Please re-evaluate the ENTIRE list based on the NEW CONTEXT below.
        
        Target Language: ${language}
        
        New User Instructions: "${newInstruction}"
        `;

        if (activeGlossaryItems.length > 0) {
            prompt += `\n\nUPDATED Glossary (Enforce these strictly):\n${activeGlossaryItems.map(g => `- ${g.term}: ${g.definition}`).join('\n')}`;
        }

        prompt += `\n\nCurrent Table State (User Edits):
        ${currentStatus}
        
        Task:
        1. Review the "UserCorrected" values. 
        2. Apply the UPDATED Glossary to fix any inconsistencies.
        3. If the User Note gives a command (e.g. "change all X to Y"), apply it.
        4. STATUS RULES:
           - If you agree with the current state or fixed it confidently -> 'corrected'
           - If it contradicts glossary/context but user insists, or if ambiguous -> 'needs_confirmation'
           - DO NOT use 'custom' or 'ai_recheck'.
        5. Return the FULL updated list.
        `;

        const response = await this.chat.sendMessage({ message: prompt });
        if (!response.text) throw new Error("No response from AI during iteration");
        return JSON.parse(response.text) as AnalysisResult;
    }
}

// ... (Rest of the file: generateSmartGlossary, fixVocabTimestamps, etc. remains unchanged) ...
// --- GLOSSARY ---

export const generateSmartGlossary = async (
  srtContent: string,
  vocabList: VocabItem[],
  modelName: string,
  language: string,
  apiKey: string,
  baseUrl: string
): Promise<GlossaryItem[]> => {
  const ai = getClient(apiKey, baseUrl);
  const vocabText = vocabList.map(v => `${v.corrected} (${v.type})`).join(', ');

  const prompt = `
  Analyze the Subtitle content and the corrected vocabulary list.
  Generate a professional glossary of key technical terms, proper nouns, and important entities.
  
  Rules:
  1. Deduplicate terms.
  2. Provide a contextual definition/explanation for how the term is used in this specific content.
  3. Output language for definitions: ${language}.
  
  Vocabulary Hints: ${vocabText}
  
  Subtitle Content Preview:
  ${srtContent.slice(0, 20000)}
  `;

  const glossarySchema: Schema = {
    type: Type.ARRAY,
    items: {
      type: Type.OBJECT,
      properties: {
        id: { type: Type.STRING },
        term: { type: Type.STRING },
        definition: { type: Type.STRING },
        context: { type: Type.STRING }
      },
      required: ["id", "term", "definition"]
    }
  };

  const response = await ai.models.generateContent({
    model: modelName,
    contents: prompt,
    config: {
      systemInstruction: SYSTEM_INSTRUCTION,
      responseMimeType: "application/json",
      responseSchema: glossarySchema
    }
  });

  if (!response.text) throw new Error("Failed to generate glossary");
  return JSON.parse(response.text) as GlossaryItem[];
};

export const generateGlossaryFromRawText = async (
    rawText: string,
    context: string,
    modelName: string,
    language: string,
    apiKey: string,
    baseUrl: string
): Promise<GlossaryItem[]> => {
    const ai = getClient(apiKey, baseUrl);

    const prompt = `
    Analyze the following raw text or file content.
    Extract key terminology, proper nouns, and technical entities.
    Structure them into a clean glossary.
    
    User Context/Background: ${context || "None"}
    Target Language for definitions: ${language}
    
    Rules:
    1. If the input is already a list (e.g. "Term: Def"), structure it.
    2. If the input is raw text, identifying meaningful terms.
    3. Generate definitions if missing, based on the context provided.
    
    Input Content:
    ${rawText.slice(0, 25000)}
    `;

    const glossarySchema: Schema = {
        type: Type.ARRAY,
        items: {
          type: Type.OBJECT,
          properties: {
            id: { type: Type.STRING },
            term: { type: Type.STRING },
            definition: { type: Type.STRING },
          },
          required: ["id", "term", "definition"]
        }
      };

    const response = await ai.models.generateContent({
        model: modelName,
        contents: prompt,
        config: {
            systemInstruction: SYSTEM_INSTRUCTION,
            responseMimeType: "application/json",
            responseSchema: glossarySchema
        }
    });

    if (!response.text) throw new Error("Failed to generate glossary from raw text");
    return JSON.parse(response.text) as GlossaryItem[];
};

// --- FIX TIMESTAMPS ---

export const fixVocabTimestamps = async (
    srtContent: string,
    vocabList: VocabItem[],
    apiKey: string,
    baseUrl: string
): Promise<VocabItem[]> => {
    const ai = getClient(apiKey, baseUrl);
    const vocabToCheck = vocabList.map(v => ({ id: v.id, original: v.original }));
    
    const prompt = `
    TASK: Calibrate Timestamps.
    I have a list of terms and an SRT file. The current timestamps for these terms might be wrong or hallucinatory.
    
    You must find the EXACT time range (Start --> End) from the SRT where the "original" term appears.
    
    Input Terms:
    ${JSON.stringify(vocabToCheck)}
    
    SRT Content:
    ${srtContent.slice(0, 30000)}
    
    Return:
    A JSON array of objects with 'id' (matching input) and 'timeRange' (e.g., '00:01:20 --> 00:01:25').
    If a term appears multiple times, use the first occurrence or the most relevant one.
    `;

    const responseSchema: Schema = {
        type: Type.ARRAY,
        items: {
            type: Type.OBJECT,
            properties: {
                id: { type: Type.INTEGER },
                timeRange: { type: Type.STRING }
            },
            required: ["id", "timeRange"]
        }
    };

    const response = await ai.models.generateContent({
        model: 'gemini-3-flash-preview',
        contents: prompt,
        config: {
            responseMimeType: 'application/json',
            responseSchema: responseSchema
        }
    });

    if (!response.text) throw new Error("Failed to fix timestamps");
    return JSON.parse(response.text) as VocabItem[];
};

// --- SUBTITLE GENERATION (REPAIR LOOP) ---

export const generatePolishedSubtitle = async (
  content: string,
  confirmedVocab: VocabItem[],
  modelName: string,
  format: string = 'srt',
  apiKey: string,
  baseUrl: string,
  onChunk: (text: string) => void
) => {
  const ai = getClient(apiKey, baseUrl);
  const vocabString = confirmedVocab.map(v => 
    `- Original: "${v.original}" -> Corrected: "${v.corrected}"`
  ).join('\n');

  const prompt = `
  Task: Reword and Polish the subtitle content.
  
  Strict Constraint:
  1. KEEP THE ${format.toUpperCase()} FORMAT EXACTLY. Do not break syntax.
  2. DO NOT CHANGE TIMECODES.
  3. Apply these corrections:
  ${vocabString}
  
  Input Subtitle (${format}):
  ${content}
  `;

  // 1. Initial Generation
  let fullGeneratedText = "";
  const result = await ai.models.generateContentStream({
    model: modelName,
    contents: prompt,
    config: {
      systemInstruction: SYSTEM_INSTRUCTION
    }
  });

  for await (const chunk of result) {
    if (chunk.text) {
        fullGeneratedText += chunk.text;
        onChunk(chunk.text);
    }
  }

  // 2. Validate Format using tool
  // This happens after stream is done to check integrity
  const validation = isValidSubtitleFormat(fullGeneratedText, format);
  
  if (!validation.valid) {
      console.warn("Subtitle Syntax Error detected, triggering AI Repair...", validation.error);
      onChunk("\n\n[System] Syntax error detected. Auto-repairing...\n");
      
      // 3. Repair Request
      const repairPrompt = `
      The previous output had syntax errors for the ${format.toUpperCase()} format.
      Error: ${validation.error}
      
      Please fix the syntax of the following content while keeping all text corrections.
      Return ONLY the valid ${format.toUpperCase()} file content.
      
      Invalid Content:
      ${fullGeneratedText}
      `;

      const repairResult = await ai.models.generateContentStream({
          model: modelName,
          contents: repairPrompt
      });
      
      let repairedText = "";
      onChunk("\n\n--- REPAIRED OUTPUT ---\n"); 
      
      for await (const chunk of repairResult) {
          if (chunk.text) {
              repairedText += chunk.text;
              onChunk(chunk.text);
          }
      }
  }
};

// --- MARKDOWN TRANSCRIPT GENERATION ---

export const generateFinalTranscript = async (
  srtContent: string, 
  confirmedVocab: VocabItem[],
  modelName: string,
  language: string,
  apiKey: string,
  baseUrl: string,
  onChunk: (text: string) => void
) => {
  const ai = getClient(apiKey, baseUrl);
  const vocabString = confirmedVocab.map(v => 
    `- Original: "${v.original}" -> Corrected: "${v.corrected}" (${v.type})`
  ).join('\n');

  const prompt = `
  Step 5 - Generate Refined Transcript
  Task: Rewrite into a clean, readable Article/Script format.
  Target Language: ${language}
  
  Rules:
  1. Remove timestamps.
  2. Group into paragraphs by speaker/topic.
  3. Apply corrections:
  ${vocabString}
  
  Subtitle Content:
  ${srtContent}
  `;

  const result = await ai.models.generateContentStream({
    model: modelName,
    contents: prompt,
    config: {
      systemInstruction: SYSTEM_INSTRUCTION,
    }
  });

  for await (const chunk of result) {
    if (chunk.text) onChunk(chunk.text);
  }
};

// --- CHAT AGENT ---

const CHAT_SYSTEM_INSTRUCTION = `
You are VerbaFlow's intelligent assistant, powered by Gemini. 
You are integrated into a professional audio/video proofreading application called "VerbaFlow".

Your Identity & Capabilities:
1. **General Assistant**: You can answer ANY broad question, explain complex concepts, write code, or chat casually, just like a standard LLM.
2. **Context Aware**: You know the user is likely working on subtitles, translation, or content creation.
3. **Helper**: You can help explain specific grammar errors, define obscure technical terms found in their audio, or offer translation suggestions.

Tone: Helpful, Professional, yet Conversational.
`;

export const chatWithAgent = async (
  history: { role: string, parts: { text: string }[] }[],
  message: string,
  modelName: string,
  apiKey: string,
  baseUrl: string,
  onChunk: (text: string) => void
) => {
  const ai = getClient(apiKey, baseUrl);
  const chat = ai.chats.create({
    model: modelName,
    history: history,
    config: { systemInstruction: CHAT_SYSTEM_INSTRUCTION }
  });

  const result = await chat.sendMessageStream({ message });
  for await (const chunk of result) {
    if (chunk.text) onChunk(chunk.text);
  }
};

export const generateSessionTitle = async (firstMessage: string, apiKey: string, baseUrl: string): Promise<string> => {
  const ai = getClient(apiKey, baseUrl);
  const response = await ai.models.generateContent({
    model: 'gemini-3-flash-preview',
    contents: `Generate a very short title (max 4 words) for a conversation starting with: "${firstMessage}"`,
  });
  return response.text?.trim() || "New Chat";
};
